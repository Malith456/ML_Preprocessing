{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>STEP: 01</u></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>STEP: 02</u></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "\n",
    "# libraries importing from scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# natural language toolkit packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
    "from nltk.stem.snowball import  EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get all the text files from the directory to a list\n",
    "def read_class_files(dir):\n",
    "    fl = []\n",
    "    for roots, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            fl.append(os.path.join(roots, file))\n",
    "    return fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = read_class_files('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to categorize and read the files\n",
    "comp_fl = []\n",
    "rec_fl = []\n",
    "sci_fl = []\n",
    "talk_fl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_files(files):\n",
    "    for file in files:\n",
    "        if \"comp\" in file:\n",
    "            comp_fl.append(file)\n",
    "        elif \"rec\" in file:\n",
    "            rec_fl.append(file)\n",
    "        elif \"sci\" in file:\n",
    "            sci_fl.append(file)\n",
    "        elif \"talk\" in file:\n",
    "            talk_fl.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorize_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp  4891\n",
      "sci 3952\n",
      "talk  3253\n",
      "rec 3979\n"
     ]
    }
   ],
   "source": [
    "print(\"comp  \"+str(len(comp_fl)))\n",
    "print(\"sci \"+str(len(sci_fl)))\n",
    "print(\"talk  \"+str(len(talk_fl)))\n",
    "print(\"rec \"+str(len(rec_fl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_train,comp_test = train_test_split(comp_fl, train_size=0.7,test_size=0.3, random_state=42)\n",
    "rec_train, rec_test = train_test_split(rec_fl, train_size=0.7,test_size=0.3, random_state=42)\n",
    "sci_train, sci_test = train_test_split(sci_fl, train_size=0.7,test_size=0.3, random_state=42)\n",
    "talk_train,talk_test = train_test_split(talk_fl, train_size=0.7,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_train_data_file =[ comp_train, rec_train, sci_train, sci_train]\n",
    "All_test_data_file =[ comp_test, rec_test, sci_test, sci_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the set of stop words\n",
    "_stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "# initialize english stemmer\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "# initialize word net lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# word list used in english language\n",
    "_word_list = set([word for word in wn.words(lang='eng')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique words\n",
    "def process_files(data_list):\n",
    "    word_set = []\n",
    "    for docs in data_list:\n",
    "        for doc in docs:\n",
    "            words = []\n",
    "            for word in word_tokenize(doc):\n",
    "                word = stemmer.stem(word)\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "                words += [word]\n",
    "            word_set += [word for word in words if word not in _stop_words if word in _word_list if word.isalpha()]\n",
    "    return list(set(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_train = process_files (All_train_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_comp = process_files(comp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_csv(cleaned_data, name, vocabulary_set):\n",
    "    vectorizer = CountVectorizer(max_features=500,\n",
    "stop_words=_stop_words, vocabulary=vocabulary_set)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    df.to_csv(name+'_freq.csv')\n",
    "    \n",
    "    print(name+'_freq.csv successfully created!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary passed to fit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-300b1706e646>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvectorize_csv\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mclean_train_comp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_comp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoc_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-67970444b564>\u001b[0m in \u001b[0;36mvectorize_csv\u001b[1;34m(cleaned_data, name, vocabulary_set)\u001b[0m\n\u001b[0;32m      2\u001b[0m     vectorizer = CountVectorizer(max_features=500,\n\u001b[0;32m      3\u001b[0m stop_words=_stop_words, vocabulary=vocabulary_set)\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    861\u001b[0m                 \"string object received.\")\n\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m         \u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_validate_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    292\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"empty vocabulary passed to fit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary passed to fit"
     ]
    }
   ],
   "source": [
    "vectorize_csv (clean_train_comp, 'train_comp', voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
